login: noe.goudian
pwd: thotheeN5d


Lundi 5 Février : 9H - 12H 13H - 18H 
Arrivée au CMI, Farah fait visiter, rdv SA matin -> pistes docs :
Keras, adversarial net, TFlow, Automates pondérés, apprentissage spectral

Mardi 6 Février :
Exam le matin, convention a Luminy aprèm,
Après pas le temps de venir.

Mercredi 7 Février : 8H - 18h30 (ns)
Python pendant deux heures, ensuite tentative d'installation de TF
Fonctionne pas, peut-être car Ubuntu 14.04, mise a niveau vers 16.04
Finalement résolution des problèmes, downgrade pip, premiers réseaux sur MNIST

Jeudi 8 Février : 8H - 17H15 (ns)
Python pendant deux heures
conférence bitcoin à midi
retours sur les réseaux MNIST -> fix erreurs dans mon parseur
maintenant je gère assez bien venv/pip
premiers réseaux sur SPICE ? fonctionne trop bien pour être honnête ?


Vendredi 9 Février : 9H30 - 14H
Spice fonctionnait bien car sur le bourage ! Maintenant il fonctionne mal comme prévu
réunion du vendredi, exposé Farah, ...
spice essais divers

--> S1 : 8+(7)+10.5+9.75+4.5 = 39.75

Lundi 12 Février : 9h00 - 12h00 13h-18h
Retouches spiceparse mais finalement pas utile, essais avec plus de neuronnes, plus 
d'epochs, toujours pas de résultats intéréssants
lecture docs sur LSTM, pas mieux ?
Mauvaises performances car retourne toujours le plus fréquent ("3"). -> Mêmes résultats 
avec 100 ou 10 ou 1 neuronnes, comment ça se fait ?
Docs TF, LSTM, ...
Discussion RE -> Caractères 1 par 1 ? fenetre glissante y compris au début
tentatives utilisation du cluster, encore des problèmes, TF 1.5 trop en avance sur CUDA ?

Mardi 13 Février : 8h00 - 12h 13h - 17h
SA me montre cluster (effectivement problème de version TF + d'autres), divers docs, 
nouveau parseur
Avec nouveau parseur efficace environ 40% sur couches taille 10, c'est mieux ! vrai 
réponses et pas juste la plus probable
En ajustant un peu longueur de fenetre et nb de neuronnes, on obtient jusqu'à 80%, mais on 
retrouve un peu le "biais de bourrage"
fenetres multiples et bourrage partout pour compenser, tests en cours.
Lancement sur cluster (asfalda1), ça va moins vite que local !?! La faute a PV ?

Mercredi 14 Février : 9h30 - 12h40 13h40 - 18h30
Mini réunion SA et RE, je continue les expérimentations spice
Local plus rapide que adnvideo1 !
Entrées en argument, on lance toutes les taches sur clusters a la fois, il faudra 
décortiquer
décortiquage : bon pas mauvais mais pas top non plus, voir papiers
Lectures très bonnes docs collah sur LSTM et RNN et autres
Lectures docs Keras sur embedding, padding -> il le fait vraisemblablement mieux que moi, 
a tester
Je lance sur le cluster pour la nuit les memes trucs mais avec les données 1. Wall sera 
suffisant ?

Jeudi 15 Février : 10h - 13h 14h - 18h30
6 sur 18 ont terminé cette nuit, Wall trop court pour les autres, ils sont décevants : 
apprentissage du plus fréquent
Pas la peine de relancer les autres, si il faut attendre deux heures, autant étudier la 
couche mbed
nouveau parseur pour aller avec l'embedding, mise en git car ça devient chiant a gérer 
tout en scp
ça marche mieux avec embed+parse3, je lance des travaux sur le cluster avant le repas, on 
décortique après manger
Après manger toujours pas fini, en fait j'avais relancé sur le jeu 1, mais celui-ci est 
trop gros, donc j'ai relancé sur jeu 0, en attendant je fais de l'algèbre et j'attaque les 
articles sur WFA
Toujours pas fini en fin d'aprem, décidement beaucoup plus long, j'ai mis a cuire pour la 
nuit on verra demain

Vendredi 16 Février : 8h30 - 12h30
Ce matin certains ont fini, d'autres sont encore en cours, ils ont commencé plus tard, on 
va extraire ce qu'on a pour commencer. ça finit a peu près au fur et a mesure
Les résultats sont globalement meilleurs, et en tout cas plus eq.
Réunion du Vendredi
Début doc SciKit Learn et SP2 Learn

--> S2 : 8+8+8+7.5+4 = 35.5

Lundi 19 Février : 8h - 12h10 13h10 - 17h
J'ai épluché les résultats sur les données 6 : plutôt bien, s'améliore toujours avec plus 
de neuronnes
Faut-il faire une expérience avec beaucoup de neuronnes ? peut être temps de passer à 
autre chose,
Je vais essayer de me consacrer au spectral cette semaine.
Réunion SA et RE : faire la métrique spéciale dans spicetest, continuer lecture spectrale
Bon la métrique ça marchera probablement pas a cause de la mise en désordre, mais on 
pourra évaluer à posteriori
Relance des expériences sur données 8 et avec fenetre "maximale"

Mardi 20 Février : 8h30 - 13h15 13h15 - 17h30
Certains n'ont pas fini, et ils risquent de déborder le walltime, merde. Il y en a un, je 
sais pas lequel qui est à 12h/epoch environ
On va essayer d'extraire le reste pour commencer.
Du coup finalement j'ai pas encore attaqué a extraire, mais j'ai préparé la métrique, on 
peut la mesurer a posteriori
ça a pas été très facile, adaptation Python2/3, bug bête de ma part, best_n, ...
Le train commence par écrire son nom dans la sortie, ça permet d'identifier les 
retardataires.
J'ai aussi mis en place une sauvegarde du modele après chaque epoch, ça évitera les 
incidents malheureux de manque de walltime a pas grand chose
Je relance des expériences avec d'autres données et toutes les améliorations mentionnées 
ci-dessus. Ensuite un peu de spectral

Mercredi 21 Février : 8h - 11h40 12h40 - 17h
Toutes les expériences sauf (-1, 400) sont terminées, j'observe les résultats : on dépasse 
plusieurs fois la référence (Shibata modèle LSTM) !
J'attends la dernière pour comparer avec les modèles les plus proches possibles (il y a 
quelques diff cependant).
Aprem spectrale et algèbre. Décomposition au rang : quel est le critère d'arrêt du pivot 
de gauss ?
Réunion avec SA et RE : On rentre dans le vif du sujet, cf photo.

Jeudi 22 Février : Conférences St-Charles

Vendredi 23 Février : 9h - 12h30
Mail RE sur l'avis de Benoit Favre, je lis les docs
installation un peu bordélique de splearn.
Réunion du vendredi.

--> S3 : 8+8+8+(8)+3,5 = 35,5 

Lundi 26 Février : 9h30 - 11h30 12h30 - 18h30
(Aujourd'hui pas de chauffage, au début ça va mais en fin de journée on commence a se 
cailler un peu le cul)
A l'attaque sur le spectral !
Je jette un oeil sur comment c'est fait dedans.
Un petit détour python POO / décorateurs pour mieux comprendre.
On commence les manips pour générer tous les mots de la bonne longueur et ainsi de suite...
Repas avec Coline, putain le crous est fermé.
Je fais le "RNN to Hankel", mais j'ai un doute sur comment le brancher avec Spectral -> 
retour a l'inspection du code -> retour a la POO Python.
RE me parle d'un article avec les memes objectifs mais une methode différente qui date de 
2012
Ok ça va mieux en python, il faudra réattaquer demain


Mardi 27 Février : ~8h
A la maison
(Putain j'ai cassé Ubuntu avec un écran HDMI)
Etude du fonctionnement de Spectral.fit() et ramifications...
(Le soir je réinstalle une 17.10, c'est cool !)

Mercredi 28 Février : ~8h
J'essaye de faire mon (9 piece luggage set) custom fit
Bon ok ça a l'air de pas trop mal marcher, j'entraine un model sur un exemple pas trop gros
pautomac train 4, on verra ce que ça donnera.

Jeudi 1 Mars : ~7h
On lance les premières expériences sur le truc, ça marche pas top, c'est peut etre car 
petit rang, lrows, lcols ...

Vendredi 2 Mars : ~4h
J'essaye d'accélérer un peu les choses, sur petits params ça marche pas, sur gros ça finit 
pas...
Je lance des grosses fournées pour le WE, on verra lundi

--> S4 : 35

Lundi 5 Mars : 9h30 - 11h30 12h30 - 18h00
Les expériences n'ont pas fini, c'est la merde.
Réunion avec RE : il faut juger du succès ou de l'échec avec la perplexité, pas a l'oeil.
Mise en place du calcul de perplexité autour de l'extraction de l'automate ...
Premières expériences : le RNN est absolument nul (1200 de perp contre 80 attendu) et 
l'automate extrait encore pire (19237 de perp...). Grosse variabilité de la perp avec 
epsilon (jusqu'à 2m) ?
En pratique le problème est que epsilon est utilisé 834/1000 fois, donc l'automate est 
moisi, dégénéré, parce que le RNN est dégénéré...
A faire expériences : plus d'épochs, padding à la fin, ... cf kw "language modelling"
Expériences en cours : avec beaucoup d'epochs, classification binaire et normale, sample 
15k.
On a pas encore toutes les epochs mais les premiers modèles donnent des résultats aussi 
nuls que d'habitude...
Dernière minute : RE suspecte un mauvais usage de la prédiction par le RNN, ce qu'on veut 
c'est le produit des probabilités de tous les suffixes sachant les précédents, et pas 
seulement du symbole de fin sachant que le mot entier est son préfixe ! Objectif de 
demain !

Mardi 6 Mars : 9h - 18h (ns yerba)
Je met en place le nouveau calcul de la probabilité d'un mot, RE avait raison, le RNN a 
maintenant une perplexité tout a fait acceptable de 87 pour 80 sur pautomac 4, par contre l'automate 
extrait est toujours à la ramasse, il utilise epsilon dans 80% des cas. Est-ce car on 
utilise petit lrows et lcols ? expériences en cours...
J'en profite pour entrainer un nouveau RNN avec de petites evolutions, petits bugs 
résolus, et sur un autre jeu de données : 5 ...
Au passage je fais une course CPU contre GPU sur le cluster, car je suspecte un problème 
avec les GPUs trop lents ? 
Il semblerait effectivement que le train aille plus vite sur CPU que GPU ! comment est-ce 
possible ? Et en plus sans compilation optimisée pour AVX, ...
Du coup j'essaye de compiler tensorflow pour AVX sur machine locale
La compilation maison de TF a fonctionné, ça va plus vite en effet, j'essaye maintenant de 
compiler pour les E5-2650 Sandybridge du cluster, mais depuis ma machine locale car
je ne peux pas installer bazel sur le cluster.
Complications inattendues : il faut build pour python3.4, qui n'est pas sur ub17.10, donc
il faut bidouiller ça prend du temps.
Entre temps j'implémente kl divergence en plus de la perplexité dans hank.py
C'est un peu le bordel avec mes venvs sur le cluster, il faut que je range tout ça
Tentatives de parallélisations de la génération des mots pour aller plus vite, malheureusement
il y a la difficulté du set, sinon les trucs pas uniques vont tuer le temps que prends le model.
En fait le set ça se passe très bien, on fait aussi les probas en un GROS batch et ça va mieux,
la vitesse est vraiment meilleure, on va pouvoir faire des tests assez gros.

Mercredi 7 Mars : 10h - 18h (ns)
Je passe la matinée a mettre de l'ordre dans les venvs sur le cluster et a essayer de compiler TF
pour les cpus du clusters. Malheureusement plein de problèmes de version de gcc, libcpp, ...
Je fais quelques essais en local pour hank.py mais même après quelques réglages, on a des problèmes
d'échelle : impossible de le faire marcher avec des taille qui laisse une chance de fonctionner.
Je repars à l'attaque de la compilation de TF cette fois en compilant Bazel sur le cluster, pour
pouvoir faire la compilation de TF de là bas (adnvideo1).
Ok compilation de Bazel longue mais ça a l'air bon
J'ai pu lancer compilation de TF CPU, un peu long, on verra.
A part ça, à partir de 3 ou 4 epochs, la perplexité du modèle augmente fortement : surapprentissage?
J'ai essayé de comprendre pourquoi on a autant de zéros, c'est un arrondi, qui se produit encore
plus fréquement sur les longs mots, forcément.

Jeudi 8 Mars : 10h - 18h (ns)
J'arrive en retard car j'ai perdu ma carté de métro. En arrivant je suis déçu de voir que ni la
compilation de TF ni l'expérience avec gros lrows et lcols sur le jeu 5 n'ont fonctionnées.
Je n'ai pas d'autre choix que de relancer (pour la compilation cette fois il semble se passer
vraiment quelquechose, ok).
Cette fois la compilation a réussi, en 10 minutes, je ne sais pas ce qu'il s'est passé hier soir...
Du coup ça y est on a un environnent performant pour mener des expériences a grande échelle !
Ok après expérience, le CPU semble plus rapide pour l'entrainement du modèle, mais pour l'extraction
GPU fait un peu mieux.
Je continue le travail sur l'accélération de hank.py : trops de doublons soumis au rnn, paralléli-
sons l'encodage-prefixage-padding des mots a évaluer.
Mini réunion SA : pointe problèmes de RNN, suggère de vérifier en croisant loss, perp, sur des
pautomacs pour vérifier que ce ne soit pas un problème de RNN inadapté.
Un réorganisation du code me semble nécéssaire à la lumière de mes progrès en Python.
Les travaux sur proba_words sont bons ! sur pautomac5, 200 neuronnes, 3 3 on passe d'un grosse heure
à 7 minutes. ça laisse espérer de pouvoir monter à 4 4, je lance l'expérience, a relancer demain
matin pour le weekend eventuellement.
Après j'améliore test selon les conseils de SA, voir test2.py
Je lance l'expérience sur test2.py, données paut6

Vendredi 9 Mars : 9h45 - 12h
Observations des résultats des expériences.

--> S5 : (7.5)+9+8+8+(2.25) = 34.75

Lundi 12 Mars : 8h30 - 11h30 12h - 17h
Je décortique un peu les résultats des expériences, quelques mauvaises nouvelles :
 - Les apprentissages (hors ceux tués pour libérer des GPUs) même avec 96h ne finiront pas, mais on
a quand même une quinzaine d'epochs sur 20.
 - L'extraction d'automate est morte aussi, je pense que c'est par manque de RAM, à creuser pour
éviter ce problème.
Par de petits calculs, problèmes confirmés : donc on a toujours un problème d'échelle pour la
spectralisation, plus a cause du temps mais à cause de l'espace. A creuser...
Donc : petite amélioration pour pas faire les mêmes calculs pours lig et col si ils ont des dims en
commun, del des trucs qui serviront plus (mieux que le ramasse-miettes ?), Encore un peu plus de par
allélisation, mais ça ne résoud pas les questions d'espace... faut-il passer a un générateur ?
Après études et réflexions, un générateur réduira fortement la contrainte d'espace, j'espère que
ça ne pèsera pas trop sur la contrainte de temps (moins de parallélisme a priori, sauf si TF/Keras
sont des génies et que predict_generator les tire au fur et a mesure du generator, auquel cas ce
sera masqué par le temps de prédiction, ce qui serait génial) et aussi que ça suffira à résoudre le
problème d'espace.

Mardi 13 Mars : 8h-12h 13h-19h45
Coupure electrique aujd 12-13h30 ça m'arrange pas du tout, travail AM maison ?
Hank2 avec générateurs fonctionne maintenant, il reste a tester. ça a été l'occasion de découvrir un
gros bug dans hankel (remplissage des matrices : la epsilon était écrassée par la 1, la dernière
jamais remplie (décalage de 1) oups !), ça élimine tous les zéros qui était remplacés par epsilon,
par contre du coup il y a une moitié de négatifs !
Ce qui est censé multithreader sur le générateur ne marche pas, les args pas picklable ? Bizarre.
Problème de mise en désordre ? Apparament bug connu de Keras mais ils s'en foutent. Certains
suggère de faire à la main en utilisant train_on_batch, mais ça me semble compliqué comme parrallé-
lisation, surtout en rapport avec problème suivant :
Autre gros problème, impossible d'instancier plusieurs modèles à partir d'un même fichier on dirait.
Du coup impossible de paralléliser sur l'alphabet. 
A midi je rentre a la maison pour continuer a bosser, a cause de la coupure d'électricité.
Test de hank2 (avec generator) : Beacoup plus lent (x 2*nalpha)
On peut palier un peu ça en faisant des batchs de batchs =)
Malheureusement, à cause du retour des redondances dans les données a évaluer, un 5 5 devrait avoir
un temps de traitement d'environ 2 mois...
Du coup hank3 hybride des deux j'espère, plutôt rapide, codage/decodage d'un mot en un entier pour
ménager la RAM. Ça à l'air de marcher, je lance cette nuit avec 5 5, on verra demain.
A faire : un mode "rapide" avec raccourcis car lrows et lcols sont égaux et "plein" ?
Je finis tard pour pas lacher l'inspiration sur hank3

Mercredi 14 Mars : 8h - 11h45 12h45 - 16h
Hank3 est lui aussi mystérieusement mort. Manque de mémoire ? bizarre car en interactif ça marchait,
Moins de mémoire en sub qu'en interactif ? En fait en interactif ça a pas remarché (see4c1)
Je vais essayer de faire un hank4 "express" qui tient compte du fait qu'on utilise toujours tous les
mots jusqu'à n.
hank4express.py est né ! Je le teste avec lrows=lcols=5: 300 heures pour l'évaluation des mots
... pénible mais c'est peut-être le prix a payer...
En fait en faisant des gros batch (1000~2000) on tombe a 150 heures, raisonnablement enviseageable.
Voyons si le GRU est plus rapide ? : Non Gru pas plus rapide, environ 140 heures, avec 50 neuronnes
seulement, par contre plus docile ? ou c'est grace a pautomac6 ? > (15 4) donne des résultats
sympas : (66.98, 69.52, 76.83), KL RNN-EXTR = 0.0232. Point positif, la performance semble croitre
avec lrows-lcols (diminution du nombre de négatifs par ailleurs), qui donne de l'espoir pour (15 5)
En attendant les résultats sur (15 4) pour comparer, je regarde le résultat des nombreux apprentis-
sages lancés vendredi. J'ai du en tuer certains pour libérer des GPUs, w8 a planté a cause d'un bug
avec le sous-padding (pas traité à ce jour, car hank3 et hank4 est devenu la norme, se méfier), la 
reférence n'a pas eu le temps d'aller au bout des 20 batchs (96h quand même !). Résultats :
Loss et perplexity ne sont pas corrélés (mais c'était pas attendu qu'ils le soient), psample est bon
en loss mais pas en perplexity, se méfier !
Réunion SA et RE :
C'est normal qu'il y ait des poids négatifs prédits par l'automate extrait, ça devrait diminuer en
augmentant le rang et la taille des préfixes et suffixes, il sera difficile de descendre en dessous
de 5~10% de négatifs.
C'est (peut-être) normal que ça marche mieux sur pauto6 (DPFA) que pauto5 (HMM) en raison de leur na
ture. (Je vais peut-être me cantonner a des exemples dociles pour avoir quelques résultats sympas)
TODO :
Essayer en variant fortement le rang, essayer en sélectionnant aléatoirement un certain nombre de 
préfixes et suffixes, ça devrait être possible en adapdant légèrement hank3. RE doit me donner une 
liste des problèmes pautomac les plus représentatifs d'après B. Balle. RE absent 1.5 semaines après

Jeudi 15 Mars : 9h30 - 18h45 (ns yerba)
Je modifie hank4 pour prendre une liste de rangs a considérer plutôt qu'un unique rang, ça permet de
grouper les fastidieuses opérations de préparation, puis de faire le spectral avec plusieurs valeurs
de rangs pour l'expérience.
Résultat d'une petite expérience : il semblerait que plus on augmente le rang, plus proche est la
perp, mais plus on a de negs (bizarre, contradictoire ?). (Même si on a largement dépassé le vrai
rang).
Je modifie hank3 pour faire un drop aléatoire sur ligs et cols, ça impacte assez fort sur les perfs
pour une longueur fixée, a voir si ça se compense en prenant une longueur plus grande mais en
abandonnant pour rapprocher du temps d'éxécution de hank4. C'est perf/temps qui compte. La mise en
batch est assez douloureuse, mais ça pourrait se paralléliser comme dans hank2.
Les résultats sont hyper mauvais dis-donc ! est-ce que j'applique une réduction trop brutale ? en un
petit quart d'heure ça fait beaucoup moins bien que hank4 en 5 minutes !
hank4 (25 3) en 5 minutes > 75.788, cible 68.621 (176 negs)
hank3 (25 4 4) compréssé au même nombre que 3 (mais ça fait plus de préfixes unique donc long) en
15 minutes environ > 1336.230, cible 68.621 (408 negs)
J'ai essayé avec des valeurs de rangs variables, ça reste désespérément foireux ! et avec plus de
longueur c'est aussi assez catastrophique.
Y a-t-il un bug ? dans certains cas je récupère une KL négative, c'est impossible normalement !
peut-être qu'a cause d'epsilon, on n'a pas une vraie dist de proba, donc les thms sur KL ne marchent
pas.
Expérience : paut6 perp réelle : 66.98, perp rnn : 68.62, taille  3 3,
			 vrai rang : 19, rangs : 8 10 19 20 30 50 100 200
Avec fausse réduction (coeff 1 1) : résultat 84 sur rang 8
								    résultats ~75 +/- 1 sur rangs 10 ... 100
								    résultat 1e+35 sur rang 200
CONFIRMATION par hank4 (sans réduction, exacts mêmes résultats que hank3 avec réduction 1 1)
Avec petite réduction (coeff 0.95 0.95) : a peu près idem (un peu mieux ?)jusqu'à rang 50 inclus, 
										  n'importe quoi total pour 100 et 200
Avec un petit coeff de réduction (95%), on a des résultats semblables a sans réduction jusqu'à rang
50 (perp 74.49, visée 68.62), plus gros rang la perplexité explose (à 100 : 761m) ! Pourtant la KL
continue de descendre ! on voit que nb de probas négatives, perplexité et div KL ne sont pas cor-	
rélés. Alors qui croire ?
Préparation de trains de models sur des problèmes représentatifs (en tout cas moi je les aime) avec
les trois types de machines génératrices représentés et des petits alphabets. Exploration du HW du
cluster pour savoir sur quels noeuds je peux lancer ou non (support AVX2 notamment)
J'en profite pour débug le calcul de perplexité avec des sous-padding (voir ci-dessus) comme ça on
pourra l'utiliser pour entrainer des modèles en surveillant leur perplexité.
Problème : Entre hank 1, 2, 3, 4 trop de duplicatas partiellements maintenus, ça m'énerve. A la lu-
mière de mes progrès en python, la façon dont est faite hank* ne me plait pas, je réorganise tout
joliement avec des classes, des héritages, des surcharges... comme ça on aura le moins possible de
code dupliqué partout qu'il faut corriger et recorriger. On inaugure les Spex (SpectralExtractors).

Vendredi 16 Mars : 9h - 12h
Encore un peu de structuration de hank en spex, lancement des apprentissages. Les apprentissages
finissent rapidement. Commentaires RE et FD sur "hank3" : il faut que l'ensemble des préfixes et/ou
(pas sûrs, a vérifier) des suffixes soit préfixe-clos (et/ou suffixe-clos). Donc ça explique que
ma sélection purement aléatoire ne fonctionne pas car casse théorèmes spectraux. A refaire. Dans un
premier temps, on pourrait faire un laché seulement sur les suffixes, ça devrait garder les préfixes
préfixes-clos et donc marcher.
Réunion du vendredi.

--> S6 : 8+(10.75)+7+(9,25)+3 = 38

Lundi 19 Mars : 10h - 11h40 12h40 - 18h30
Objectifs : Faut voir les résultats des apprentissages de vendredi, continuer la réforme du code
pour obtenir un joli hank3, hank4, et les tester sur les modèles de vendredi.
Rapide point avec RE, on fixe Rdv a 17h, ça devrait me laisser le temps de faire les objectifs.
Les apprentissages de vendredi ont bien marché, mais certains auraient mérité 30 epochs de plus.
Je relance pour en avoir le coeur net. Finalement c'est parfois mieux mais surtout très variable
avec l'initialisation on dirait, je ne retrouve pas du tout les mêmes résultats entre le run avec 20
epochs et les 20 premières épochs du run avec 50 epochs. Cependant 50 epochs c'est mérité, et on
peut largement se le permettre en terme de temps de calcul.
A faire un de ces jours : ranger ma chambre sur le cluster, je crois que ça commence a être blindé.
ça nous donne quelques beaux résultats bien tape à l'oeil =) ! :
	- paut3  (maxgram, 50n, ep19, s20k) express, 4x4, r25 :
		target : 49.95		rnn : 52.21		extr : 52.63		KL rnn-extr : 0.00063
	- paut32  (8gram, 30n, ep35, s20k) express, 5x5, r50 :
		target : 32.61		rnn : 34.91		extr : 35.45		KL rnn-extr : 0.00108
	- paut28 (8gram, 30n, ep32, s20k) express, 4x4, r35:
		target : 52.74		rnn : 53.60		extr : 53.96		KL rnn-extr : 0.00009
Donc la technique marche ? On dirait que oui, reste l'interprétation visuelle de l'automate.
D'ailleurs finalement ça marche même sur les HMM, et plutot bien.
Un peu de rangement du projet, qui en avait bien besoin, un peu de rangement sur cluster, mais le
chantier est énorme, a reprendre plus tard. Hank3 et 4 sont de belles classes, les autres versions
sont pour l'instant mises aux oubliettes.
Réunion RE : 
Globalement les résultats obtenus sont encourageants,
Notre KL n'est pas vraiment une KL car pas sur la distribution entière mais seulement sur l'ens. de
test, on pourrait faire plus rigoureux,
2 branches de stage pour continuer le travail actuel. Il faudra faire un choix.

Concernant hank3, réduire seulement les suffixes ne fonctionne pas bien (perp ~300, pour une cible
de ~30) a retravailler.

Mardi 20 Mars : 10h30 - 11h40 12h40 - 18h (Jour de flemme)
Je fait enfin un truc bien carré pour vérifier la p-closure dans hank3, ok ça marche, performances
correctes (en terme de temps que ça prend de générer les préfixes) par contre ça donne pas des résu-
ltats aussi bon que hank4 pour un temps imparti donné. résultats décents, dans le bon ordre de
grandeur mais pas très pointus finalement. Quoique en choisissant bien les params, on arrive a des
résultats intéréssants. Je fais quelques petites opti pour gagner du temps dans hank3 pour le tester
plus.
Je découvre que pour faire de la parrallélisation efficace sur le cluster, il faut réserver
plusieurs coeurs ! (oarsub -l /core=2,walltime=4) Mais étrangement même en réservant plusieurs
coeurs, les gains sont très limités, même en local.
Erreur bizarre en essayant de charger un modèle Keras : apparament, les modèles enregistrés ne sont
pas compatibles entre keras 2.1.4 et 2.1.5

Mercredi 21 Mars : 8h - 18h30 (ns yerba) (il neige)
Test "visuel" sur les automates extraits. J'en profite pour faire une course entre cluster-gpu et ma
machine, c'est moi qui gagne a nouveau.
Attention la visualisation peut etre assez longue a calculer (trouver un layout sans croisements ?)
Il faut mettre du threshold sinon c'est illisible (graphe alphabet*complet).
Le résultat est assez différent, mais peut-être est-ce équivalent ? difficile de juger à l'oeil.
Il faut débiaiser KL div pour avoir une mesure objective de la ressemblance.
Travaux sur le débiaisage assez importants, un tas de spaghetti, des noms de variables pas très 
cohérents, etc.
En passant RE donne pointeurs vers d'autres mesures : distance L2 entre automates, KL-symétrique 
entre automate
Ok, le débiaisage est fini, par contre sur des mots aléatoires, le model retourne une proba de 0
dans 75% des cas, donc la valeur choisie pour epsilon dicte très fortement la KL sur rand, donc elle
n'est pas pertinente... Ou alors peut-être que oui ? que non ? je suis perdu.
En fait si P (la distribution de référence) contient des 0, c'est pas grave, c'est prévu par KL.
C'est Q (la distribution approximant P) qui ne doit pas contenir de 0, ou alors P donne zéro aussi
pour ce mot. Dans mon cas ça arrive parfois que Extr soit neg (donc 0, donc epsilon ?)
Finalement il me semble que ça marche, et alors on aurait de très bon résultats dis-donc !
Réunion RE : A voir WER, KL a l'envers, on arrive a se convaincre que les résultats ont du sens.

Jeudi 22 Mars : 11h - 18h30 (ns yerba)
Je me renseigne sur WER, mais dans B. Balle les détails sont maigres, et ailleurs ça veut dire autre
chose. Je raffine un peu les mesures KL et Perp en indicant la proportion d'epsilon utilisée dans
chaque calcul. c'est plus clair, plus net, ça permet de savoir quels nombres sont fiables ou trop
dépendants d'epsilon.
Recherches et implé de WER. On peut utiliser WER sur l'ensemble de test mais pas sur l'ensemble de
mots aléatoires ? ça semble absurde. On alors sur les mots aléatoires mais transformés en "ce que
prédit [RNN/Model] a partir de ce préfixe" pour voir si ils font les mêmes erreurs. Ou alors générer
des mots avec le RNN et tester la WER de l'automate extrait la dessus. Implé de WER sur automate est
faite, faut-il refaire un truc pour RNN ou faire en simultané un peu compliqué avec proba_words ?

Vendredi 23 Mars : 9h-12h
WER pour les RNN aussi, finalement pas si compliqué de le faire en même temps que l'évaluation.
Réunion du vendredi
Rapide réunion RE :
Valide la métrique WER(mots générés par l'automate - automate extrait sur ces mots). à implémenter.
Les résultats sont assez bons, a lancer sur tous les problèmes pautomacs pour de bon.
A faire éventuellement : sauvegarde du SPEX, pour pouvoir relancer avec d'autres rangs plus tard, et
figer l'ensemble aléatoire de mots.
Apprendre Latex, cf Overleaf.

--> S7 : (7.5)+(6.5)+(10.5)+(7.5)+3 = 35

Lundi 26 Mars : 9h - 12h30 11h30 - 18h
Plutot que générer les mots aléatoirement avec le rnn, on va utiliser les mots aléatoires comme
'seed', comme ça pas plus de comportement aléatoire, cf WER du model sur ses propres générations qui
est assez mauvaise. Donc test des 'mêmes erreurs'.
Ce test donne une erreur d'un peu plus de 60%, c'est pas bon ! On a mis le doigt là ou ça fait mal ?
Envisager un calcul a la manière du score de Spice ? (NDCG_5)
Peut-être que ça signifie que l'extraction fonctionne bien pour les mots de forte probabilité, mais
pas aussi bien sur les mots a faibles probabilités. Compatible avec l'intuition de ce que fait le
spectral ?
Réunion avec SA :
Technique 'mêmes erreurs' discutable, car mots très improbables voire interdits. Préférer générateur
mais alors sommes nous représentatifs de toute la distribution ? Plutôt oui.
Envisager d'améliorer le RNN.
J'implémente le générateur de mots de RNN, effectivement le RNN lui même fait 49% de fautes sur les
mots qu'il a lui-même générés ! Du coup la comparaison devient difficile.
Je travaille à implémenter NDCG pour mieux jauger.

Mardi 27 Mars : 10h - 18h (ns yerba)
(aller retour car oubli ordinateur), du coup travail effectif a 11h30
Accélérer ndcg, vérifier pertinence, implémenter partout proprement.
Avec un dico ça va un peu plus vite en effet, mais c'est toujours un peu lent, a cause du RNN.
La mesure ndcg-1 donne des résultats meilleurs que WER, la solution pour vaincre l'aléatoire ? en
effet ndcg-1 correspond a compter le nombre de fois qu'on a prédit le symbole le plus probable comme
prochain symbole et pas forcément le vrai prochain, dans lequel vient se mêler un aléatoire qu'on ne
peut pas prédire. Métrique pertinente je pense. Attention cependant, entre WER et NDCG l'échelle est
inversée, faut-il présenter 1-NDCG ou 1-WER ? J'ai choisi 1-WER.
Ok avec NDCG:1 les résultats sont pas mal.
A faire a l'occasion : faire une politique de nommage plus stricte et uniforme sur tous les attribs
de métriques.
Premiers essais naïfs de pickle, ça marche pas du premier coup, a réessayer plus en détails si c'est
vraiment nécessaire.
Je fais des essais avec les autres problèmes pré-sélectionnés : 3 mais aussi 28 et 32. Les résultats
sont un peu moins bons, mais toujours honorables (ndcg:1 ~ 90% entre extr et mots générés rnn).
Est-ce qu'on passe à la phase de test grandeur nature ?
Rangements dans le git, ajout de RE.
Rangements cluster pour préparer grands tests -> c'est brutal mais si ça prend plus longtemps de
retrouver le résultat d'un expérience que de la refaire à quoi bon garder les traces ?
Tentative de lier poids de l'mbed et de la classif finale pour améliorer RNN -> oui mais leur shape
sont très différentes, notament a cause du 0 magique de bourrage, faut-il décaler aussi les sorties?
De plus ça impose de faire un mbed sur un vecteur de taille neurons/2, pour retomber sur la meme
forme que la classif finale. Ou alors ajouter encore une couche à la fin. Dans tous les cas il
faudra faire une transposition pour adapter les choses, du coup on casse le lien de pointeur ?
Ah ben non en numpy transpose garde le lien de pointeur, les modifs sont répercutées.
A demander a SA demain.

Mercredi 28 Mars : 10h - 18h30 (ns yerba)
Je continue a travailler a améliorer un peu le rnn, demain au plus tard il faudra lancer les expés !
[
Bug rigolo dans PyCharm : un fichier nommé "parse4.py" n'est pas considéré comme python, ne peut pas
être run et tout. Même testé dans un nouveau projet ça foire. Grand Mystère !
]
Je consulte SA qui recommande de réécrire les fichiers décalés de 1 une bonne fois pour toutes.
Mais ça pose problème a cause des raccourcis pris un supposant que les mots sont les entiers de 0 a
un certain N, cf hank4. ça remettrait en cause beaucoup de choses. Donc il vaut peut-être mieux
retraiter systématiquement le retour de model.predict(...) en enlevant les colonnes indésirables.
Voire y compris la colonne "symbole début" et renormaliser les valeurs ?
Au passage découverte d'un bug dans le vieux test.py, le symbole de fin est pour moi nalpha+1, pour
la fonction d'évaluation par contre c'est -1 !
Au final imposer des poids égaux c'est plus galère que prévu, ça demande de créer une classe custom
qui hérite de layer, et de faire ses tripatouilles dedans.
Ou alors il y a peut etre une solution en mettant les doigts profonds dans la prise private, je me
renseigne dessus.
Finalement c'est mort cette méthode, je retourne vers la class qui hérite de keras.layers.Dense
Finalement, c'est mieux d'hériter de keras.layers.Embedding. sinon gradient vanish !
Malheureusement ça ne fonctionne pas du tout, call est appelé une unique fois, donc pas de maj des
poids de la couche. Ah oui en fait c'est normal, cf fonctionnement graph de TF.
Bon ça aura pris presque toute la journée mais au final c'est enfin réussi, avec l'aide de SA
les poids de l'embed refletent ceux de la dernière couche de classif. En termes de perfs ça crève
pas les yeux, a tester plus en profondeur dès que possible.
Pistes de continuation évoquées par SA:
 - Améliorer RNN en lui donnant l'automate comme superviseur.
 - Présenter un automate très compréssé, donc lisible, comme approx (faible) du RNN
 - Montrer que les calculs sont plus rapides sur automates -> IA embarquée, basse conso


Jeudi 29 Mars : 10h30 - 18h (ns yerba)
Nettoyage des gravats des travaux d'hier, gros apprentissages à venir.
Accélération nette de ndcg (faster_ndcg), rangements du code, factorisations.
[
Enigme : Mon RNN est censé etre stateless, et pourtant, l'ordre dans le batch semble avoir un impact
infinitésimal sur le résultat. Expérience preuve : évaluer pref () seul, au millieu d'un batch, puis
seul a nouveau : les deux valeurs en seul sont égales, l'autre diffère un tout petit peu.
Assez pour foirer un assert en tout cas.
]
Je fais le générateur de mots sur automates pour créer de nouveaux ensembles de test comme préconisé
par SA. Ok c'est fait, ça marche, c'est OK, on les trouve sous le nom "devtest".
Je lance des tests sur le cluster pour essayer de voir si oui ou non le partage de poids entre mbed
et classif est bénéfique. Je teste sur pauto 10, avec des paramètres avec un peu d'envergure pour
avoir de quoi trancher. Au passage utiliser 4 cores fait en effet gagner du temps sur le rnn.
Malheureusement je vais manquer de temps pour lancer les apprentissages sur tout les jeux de données
Pautomac, il faudra vraiment le faire demain pour le WE.

Vendredi 30 Mars : 9h30 - 12h30
Un bug a traquer dans proba_words_2 ? Le calcul de perplexité part en couille totalement on dirait.
Du coup j'aurais pas le temps de lancer les apprentissages souhaités, je vais devoir le faire ce WE.
Remise en doute de la stratégie d'apprentissage, quels métriques, quels critères ? C'est un peu le
bazar. D'ailleuirs le lien mbed-dbed n'a toujours pas prouvé son efficacité, et ça me fait un critè-
re de plus a prendre en compte, alors que j'ai 48 problèmes a traiter, ça va déboucher sur plusieurs
milliers de modèles, ça me semble pas raisonnable.
En y repensant il était absolument normal que loss et perplexité ne soient pas corrélés, car c'était
la loss sur le train, qui se sur-apprend. Donc SA me fait calculer la loss sur le même que la perp.
Une vraie loss, on espère que ça sera corrélé cette fois. C'est pas parfaitement le cas mais c'est
proche en tout cas (biais sur la quantité tirée aléatoirement ?)

--> S8 : 8+8+(8.5)+(7.5)+3 = 35

Lundi 2 Avril : LUNDI DE PAQUES

Mardi 3 Avril : 11h - 11h30 12h30 - 13h + maison
Journée difficile de bosser, mal a la tete, je me perd dans trop d'objectifs.
Quelques rangements et refactoring dans les tests.
D'après quelques expériences, il semblerait que la loss de test soit a peu près corrélée au final,
on va donc laisser tomber la perplexité comme métrique pendant l'apprentissage, comme ça gain de
temps et plus grande rigueur de la démarche.
On se fait expulser car visioconférence dans le "bureau".

Mercredi 4 Avril : 9h30 - 17h30 (ns yerba)
Quelques rangements, passage définitif de parse3 à parse5, élégage des choses inutiles dans parseX.
On va mettre a plat bien propre le test pour le lancer enfin !
Arpès quelques préparations, je pense qu'on a de bonne conditions pour faire les tests, je lance sur
les 10 premiers problèmes, pour commencer, si j'arrive a obtenir des résultats exploitables je lance
-rais sur les 38 autres.
Lancés sur 2/4 machines pour pas trop squatter le cluster, cependant il semblerait que ça soit malin
et que par défaut il y ait un cpu laissé libre pour aller avec chaque gpu, auquel cas pas besoin de
se restreindre, on peut tout défoncer, et tant pis pour PV 
(2/4 car typo 'adfalda1' ou lieu de 'asfalda1' attention au prochain copié-collé de cette commande;
quelle idée aussi de mettre des noms aussi bizarres que asfalda)
Il reste a cerner le travail a faire pendant que les apprentissages mijottent pour passer a la suite
Première idée, traquer ce peut-être-bug entre proba_para et proba_2
Outils de parsing pour obtenir plus rapidement les résultats ? bof c'est jamais que tail a regarder
sur quelques (6 pour la première fournée) logs par pb.
Au fait on a pas guetté une éventuelle meilleure corrélation entre perp et métrique categ_acc.
Petit bug idiot dans train2, on repart à la case départ et on relance...
Concernant la différence entre proba_words_2 et proba_words_para, Elle est minime, difficile a
cerner, elles renvoient des résultats différents pour certains préfixes (qui peuvent être n'importe
quoi, pas de valeur particulière) et ces mêmes préfixes prédits de manière indépendante par le model
donne raison parfois à l'une, parfois à l'autre. Au vu des mauvaises performances du multithreading,
de la complexité supplémentaire inutile induite par le stockage de préfixes tronqués au padding dans
proba_words_para, j'ai décidé de détruire proba_words_para. Comme ça pas de doublon qui sème la con-
fusion.
Dans la foulée, autres rangements divers.
Début des éclaircissements sur LaTeX. Overleaf marche mal, on peut créer un compte mais pas de mdp ?
Amélioration du petit script kill all OAR.

Jeudi 5 Avril : 9h - 16h (ns yerba)
Petits problèmes réseaux la première heure.
Tous les aprrentissages ont terminé, en une nuit. Donc on pourrait se permettre un peu plus de rech-
erche de bons paramètres pour la fournée suivante.
Les loss sont toujours dans une fourchette sérrée (+/- 0.01 environ) par contre d'un problème à un
autre c'est le grand écart, de 0.36 à 2.11 (x5.8).
Catastrophe, chargement du modèle impossible a cause de la couche custom ! Pourtant j'étais au cou-
rant du risque, il me semblait l'avoir testé ?
Du coup on va passer à une méthode type load_weight, et réinstancier des modèles creux, mais c'est
nettement plus chiant. Ou alors on laisse tomber la couche maison ? je relance sans couche maison
pour trancher. J'essaye de tirer les poids seuls du fichier de sauvegarde de modèle pour que tout ne
soit pas a jeter, mais c'est pas promis que ça fonctionne.
Bon du coup j'ai refait pas mal de trucs autour de train2, fusion des deux programmes, parametre
pour dire mbed-dbed ou non, sauvegardes et chargements qui vont avec.
Cluster assez encombré, il faudra être patient.
Du coup encore un peu de Latex pour m'occuper en attendant. Bon j'ai pas l'impression qu'il y ait
tant de choses a apprendre a priori, on fera en fonction des besoins lors de la rédaction. 
Scripts pour gagner du temps en décorticage de données la prochaine fois.

Vendredi 6 Avril : 8h - 12h
Tout n'as pas eu le temps de finir, certains ont commencé il y a seulement 1h. Mais au moins tout le
monde est bien lancé, c'est toujours ça de fait. Effectivement la prochaine fois je lancerais aussi
sur see4c1, il garde bien des core cpus libres pour aller avec les gpus donc pas de pitié.
Du coup il me faut attendre, laisser le cluster faire, essayer de trouver du temps samedi aprem pour
lancer la fournée suivante ?
Du coup matinée commentaires et code propre, c'est toujours utile.

---> S9 :

Lundi 9 Avril : 13h20 - 18h10
Arrivée un peu plus tard que prévu, en voiture.
Mail de Guillaume Rabusseau transmis par RE avec théorie et implé efficace de dist l2 entre automs.
Résultats exprériences apprentissages : Dbed prouve (enfin) son utilité : dans certains cas (minori-
té) ça diminue un petit peu, dans les autres ça améliore assez nettement. Donc la version sans sera
abandonnée pour la suite. Les loss sont toujours dans une fourchette assez sérrée pour un pb donné.
Et assez variable d'un pb a un autre. C'est normal d'après RE, c'est fait exprès qu'il y ait des pbs
faciles et difficiles.
Essais d'extractions a partir des modèles appris, pour vérifier que tout soit bon avant de lancer la
suite des apprentissages : malheureusement nombreux petits bugs et désagréments (temps looooooong)
intruoduits pas les modifs récentes ? corrections et perfectionnements. Encore plus bizarre, un bug
quand j'exécute dans le terminal, mais pas dans le debugger PyCharm ?!
Non c'est bon j'ai trouvé, c'était ma faute, et c'était très con.

Mardi 10 Avril : 9h - 11h40 12h40 - 18h30
Les apprentissages suivent leur cours, a 9h50, 84 sur 228 ont fini.
Je vais essayer de plus utiliser les dicos autour de ndcg et autres pour accélérer les calculs.
Gains de temps, mais usage de mémoire (mais bon c'est sur les ensemble de test donc c'est fixe et
d'un ordre de grandeur raisonnable). Je pense que c'est une bonne chose.
Je rajoute le calcul de dist l2 entre automates fourni par GR. Du coup c'est seulement entre vrai
automate et automate extrait, par contre quel résultat est bon ? maauvais ? je manque de comparaison
Dans l'article cité, seulement aspects théoriques. Mais bon ça coute pas cher a calculer.
Sans surprise, pour un problème avec un gros alphabet (pb 10, nalpha = 11) on est cantonnés a des
mots très courts, sinon temps explosif, donc ça marche pas terrible. Du coup il faut se tourner vers
hank3, mais conso RAM explosive pour rien. Donc travaux pour améliorer hank3, il doit etre possible
de faire bien mieux que générer tous les préfixes avant d'en piocher a peine 1% aléatoirement. Il fa
ut les choper au hasard en fonction des besoins.
Il est possible de calculer le code du prefixe directement a partir du code Hush d'un mot, piste que
je pense intéréssante pour accélérer tout ça.

Mercredi 11 Avril : 8h - 17h (ns)
Les apprentissages ont presque fini, plus que 5 !
Je continue le travail sur hank3 et plus particulièrement Hush, notament calcul de préfixes a partir
utilisant le code ; sans repasser par le mot.
Ok marche bien, éléments prometteurs !
Grandes découvertes :
C'est connu que threading ne fonctionne pas bien en python (GIL) sauf quand les I/O sont le facteur
limitant. Le vrai usage multicoeur c'est multiprocessing, par contre il faut que les args soient
picklables. Véritable usage 100% sur tous les coeurs en même temps. 
La méthode union de set renvoie une copie, donc ça prend du temps pour rien, l'usage que je veux en
faire c'est plutot la methode update. Gains notables de performances.
Bon en allayt chercher divers amélioration partout, on a maintenant un hank3 très correct, qui va
nous servir pour les problèmes avec beaucoup d'états.
15h17 => Les apprentissages sont finis, je rends son cluster a PV =)
Hank3 bien amélioré. C'est Ok. Seul point a vérifier : passage des vrais mots et pas de leur code à
splearn ? C'est nécéssaire je crois. Et manière de compter les préfixes aléatoires.
Réunion avec SA et RE :
Passer les données au moulin pour de vrai.
Explorer Représentation en deux dims : PCA ?

Jeudi 12 Avril : 8h -
Effectivement, attention a passer les mots non encodés à splearn.

Vendredi 13 Avril : (sncf)

Lundi 16 Avril :
Mardi 17 Avril :
Mercredi 18 Avril :
Jeudi 19 Avril :
Vendredi 20 Avril :

Lundi 23 Avril :
Mardi 24 Avril :
Mercredi 25 Avril :
Jeudi 26 Avril :
Vendredi 27 Avril :

Lundi 30 Avril : (Maison car polo demain ?)
Mardi 1 Mai : FETE DU TRAVAIL (Polo)
Mercredi 2 Mai :
Jeudi 3 Mai :
Vendredi 4 Mai : (sncf)

Lundi 7 Mai :
Mardi 8 Mai : ARMISTICE 45
Mercredi 9 Mai :
Jeudi 10 Mai : ASCENSION
Vendredi 11 Mai :

Lundi 14 Mai :
Mardi 15 Mai :
Mercredi 16 Mai :
Jeudi 17 Mai : GT?
Vendredi 18 Mai : GT (sncf)

Lundi 21 Mai : PENTECOTE
Mardi 22 Mai : GT
Mercredi 23 Mai : GT?
Jeudi 24 Mai :
Vendredi 25 Mai :

Lundi 28 Mai :
Mardi 29 Mai :
Mercredi 30 Mai :
Jeudi 31 Mai :
Vendredi 1 Juin :

Lundi 4 Juin :
Mardi 5 Juin :
Mercredi 6 Juin : RESUME RAPPORT
Jeudi 7 Juin :
Vendredi 8 Juin : (sncf)

Lundi 11 Juin : RAPPORT DEFINITIF ?
Mardi 12 Juin : RAPPORT DEFINITIF ?
Mercredi 13 Juin :
Jeudi 14 Juin :
Vendredi 15 Juin :

Lundi 18 Juin :
Mardi 19 Juin : SOUTENANCE
Mercredi 20 Juin :
Jeudi 21 Juin :
Vendredi 22 Juin : (sncf)

Lundi 25 Juin :
Mardi 26 Juin :
Mercredi 27 Juin :
Jeudi 28 Juin :
Vendredi 29 Juin :

