login: noe.goudian
pwd: thotheeN5d


Lundi 5 Février : 9H - 12H 13H - 18H 
Arrivée au CMI, Farah fait visiter, rdv SA matin -> pistes docs :
Keras, adversarial net, TFlow, Automates pondérés, apprentissage spectral

Mardi 6 Février :
Exam le matin, convention a Luminy aprèm,
Après pas le temps de venir.

Mercredi 7 Février : 8H - 18h30 (ns)
Python pendant deux heures, ensuite tentative d'installation de TF
Fonctionne pas, peut-être car Ubuntu 14.04, mise a niveau vers 16.04
Finalement résolution des problèmes, downgrade pip, premiers réseaux sur MNIST

Jeudi 8 Février : 8H - 17H15 (ns)
Python pendant deux heures
conférence bitcoin à midi
retours sur les réseaux MNIST -> fix erreurs dans mon parseur
maintenant je gère assez bien venv/pip
premiers réseaux sur SPICE ? fonctionne trop bien pour être honnête ?


Vendredi 9 Février : 9H30 - 14H
Spice fonctionnait bien car sur le bourage ! Maintenant il fonctionne mal comme prévu
réunion du vendredi, exposé Farah, ...
spice essais divers

--> S1 : 8+(7)+10.5+9.75+4.5 = 39.75

Lundi 12 Février : 9h00 - 12h00 13h-18h
Retouches spiceparse mais finalement pas utile, essais avec plus de neuronnes, plus 
d'epochs, toujours pas de résultats intéréssants
lecture docs sur LSTM, pas mieux ?
Mauvaises performances car retourne toujours le plus fréquent ("3"). -> Mêmes résultats 
avec 100 ou 10 ou 1 neuronnes, comment ça se fait ?
Docs TF, LSTM, ...
Discussion RE -> Caractères 1 par 1 ? fenetre glissante y compris au début
tentatives utilisation du cluster, encore des problèmes, TF 1.5 trop en avance sur CUDA ?

Mardi 13 Février : 8h00 - 12h 13h - 17h
SA me montre cluster (effectivement problème de version TF + d'autres), divers docs, 
nouveau parseur
Avec nouveau parseur efficace environ 40% sur couches taille 10, c'est mieux ! vrai 
réponses et pas juste la plus probable
En ajustant un peu longueur de fenetre et nb de neuronnes, on obtient jusqu'à 80%, mais on 
retrouve un peu le "biais de bourrage"
fenetres multiples et bourrage partout pour compenser, tests en cours.
Lancement sur cluster (asfalda1), ça va moins vite que local !?! La faute a PV ?

Mercredi 14 Février : 9h30 - 12h40 13h40 - 18h30
Mini réunion SA et RE, je continue les expérimentations spice
Local plus rapide que adnvideo1 !
Entrées en argument, on lance toutes les taches sur clusters a la fois, il faudra 
décortiquer
décortiquage : bon pas mauvais mais pas top non plus, voir papiers
Lectures très bonnes docs collah sur LSTM et RNN et autres
Lectures docs Keras sur embedding, padding -> il le fait vraisemblablement mieux que moi, 
a tester
Je lance sur le cluster pour la nuit les memes trucs mais avec les données 1. Wall sera 
suffisant ?

Jeudi 15 Février : 10h - 13h 14h - 18h30
6 sur 18 ont terminé cette nuit, Wall trop court pour les autres, ils sont décevants : 
apprentissage du plus fréquent
Pas la peine de relancer les autres, si il faut attendre deux heures, autant étudier la 
couche mbed
nouveau parseur pour aller avec l'embedding, mise en git car ça devient chiant a gérer 
tout en scp
ça marche mieux avec embed+parse3, je lance des travaux sur le cluster avant le repas, on 
décortique après manger
Après manger toujours pas fini, en fait j'avais relancé sur le jeu 1, mais celui-ci est 
trop gros, donc j'ai relancé sur jeu 0, en attendant je fais de l'algèbre et j'attaque les 
articles sur WFA
Toujours pas fini en fin d'aprem, décidement beaucoup plus long, j'ai mis a cuire pour la 
nuit on verra demain

Vendredi 16 Février : 8h30 - 12h30
Ce matin certains ont fini, d'autres sont encore en cours, ils ont commencé plus tard, on 
va extraire ce qu'on a pour commencer. ça finit a peu près au fur et a mesure
Les résultats sont globalement meilleurs, et en tout cas plus eq.
Réunion du Vendredi
Début doc SciKit Learn et SP2 Learn

--> S2 : 8+8+8+7.5+4 = 35.5

Lundi 19 Février : 8h - 12h10 13h10 - 17h
J'ai épluché les résultats sur les données 6 : plutôt bien, s'améliore toujours avec plus 
de neuronnes
Faut-il faire une expérience avec beaucoup de neuronnes ? peut être temps de passer à 
autre chose,
Je vais essayer de me consacrer au spectral cette semaine.
Réunion SA et RE : faire la métrique spéciale dans spicetest, continuer lecture spectrale
Bon la métrique ça marchera probablement pas a cause de la mise en désordre, mais on 
pourra évaluer à posteriori
Relance des expériences sur données 8 et avec fenetre "maximale"

Mardi 20 Février : 8h30 - 13h15 13h15 - 17h30
Certains n'ont pas fini, et ils risquent de déborder le walltime, merde. Il y en a un, je 
sais pas lequel qui est à 12h/epoch environ
On va essayer d'extraire le reste pour commencer.
Du coup finalement j'ai pas encore attaqué a extraire, mais j'ai préparé la métrique, on 
peut la mesurer a posteriori
ça a pas été très facile, adaptation Python2/3, bug bête de ma part, best_n, ...
Le train commence par écrire son nom dans la sortie, ça permet d'identifier les 
retardataires.
J'ai aussi mis en place une sauvegarde du modele après chaque epoch, ça évitera les 
incidents malheureux de manque de walltime a pas grand chose
Je relance des expériences avec d'autres données et toutes les améliorations mentionnées 
ci-dessus. Ensuite un peu de spectral

Mercredi 21 Février : 8h - 11h40 12h40 - 17h
Toutes les expériences sauf (-1, 400) sont terminées, j'observe les résultats : on dépasse 
plusieurs fois la référence (Shibata modèle LSTM) !
J'attends la dernière pour comparer avec les modèles les plus proches possibles (il y a 
quelques diff cependant).
Aprem spectrale et algèbre. Décomposition au rang : quel est le critère d'arrêt du pivot 
de gauss ?
Réunion avec SA et RE : On rentre dans le vif du sujet, cf photo.

Jeudi 22 Février : Conférences St-Charles

Vendredi 23 Février : 9h - 12h30
Mail RE sur l'avis de Benoit Favre, je lis les docs
installation un peu bordélique de splearn.
Réunion du vendredi.

--> S3 : 8+8+8+(8)+3,5 = 35,5 

Lundi 26 Février : 9h30 - 11h30 12h30 - 18h30
(Aujourd'hui pas de chauffage, au début ça va mais en fin de journée on commence a se 
cailler un peu le cul)
A l'attaque sur le spectral !
Je jette un oeil sur comment c'est fait dedans.
Un petit détour python POO / décorateurs pour mieux comprendre.
On commence les manips pour générer tous les mots de la bonne longueur et ainsi de suite...
Repas avec Coline, putain le crous est fermé.
Je fais le "RNN to Hankel", mais j'ai un doute sur comment le brancher avec Spectral -> 
retour a l'inspection du code -> retour a la POO Python.
RE me parle d'un article avec les memes objectifs mais une methode différente qui date de 
2012
Ok ça va mieux en python, il faudra réattaquer demain


Mardi 27 Février : ~8h
A la maison
(Putain j'ai cassé Ubuntu avec un écran HDMI)
Etude du fonctionnement de Spectral.fit() et ramifications...
(Le soir je réinstalle une 17.10, c'est cool !)

Mercredi 28 Février : ~8h
J'essaye de faire mon (9 piece luggage set) custom fit
Bon ok ça a l'air de pas trop mal marcher, j'entraine un model sur un exemple pas trop gros
pautomac train 4, on verra ce que ça donnera.

Jeudi 1 Mars : ~7h
On lance les premières expériences sur le truc, ça marche pas top, c'est peut etre car 
petit rang, lrows, lcolw ...

Vendredi 2 Mars : ~4h
J'essaye d'accélérer un peu les choses, sur petits params ça marche pas, sur gros ça finit 
pas...
Je lance des grosses fournées pour le WE, on verra lundi

--> S4 : 35

Lundi 5 Mars : 9h30 - 11h30 12h30 - 18h00
Les expériences n'ont pas fini, c'est la merde.
Réunion avec RE : il faut juger du succès ou de l'échec avec la perplexité, pas a l'oeil.
Mise en place du calcul de perplexité autour de l'extraction de l'automate ...
Premières expériences : le RNN est absolument nul (1200 de perp contre 80 attendu) et 
l'automate extrait encore pire (19237 de perp...). Grosse variabilité de la perp avec 
epsilon (jusqu'à 2m) ?
En pratique le problème est que epsilon est utilisé 834/1000 fois, donc l'automate est 
moisi, dégénéré, parce que le RNN est dégénéré...
A faire expériences : plus d'épochs, padding à la fin, ... cf kw "language modelling"
Expériences en cours : avec beaucoup d'epoches, classification binaire et normale, sample 
15k.
On a pas encore toutes les epochs mais les premiers modèles donnent des résultats aussi 
nuls que d'habitude...
Dernière minute : RE suspecte un mauvais usage de la prédiction par le RNN, ce qu'on veut 
c'est le produit des probabilités de tous les suffixes sachant les précédents, et pas 
seulement du symbole de fin sachant que le mot entier est son préfixe ! Objectif de 
demain !

Mardi 6 Mars : 9h - 18h (ns yerba)
Je met en place le nouveau calcul de la probabilité d'un mot, RE avait raison, le RNN a 
maintenant une perplexité tout a fait acceptable de 87 pour 80 sur pautomac 4, par contre l'automate 
extrait est toujours à la ramasse, il utilise epsilon dans 80% des cas. Est-ce car on 
utilise petit lrows et lcols ? expériences en cours...
J'en profite pour entrainer un nouveau RNN avec de petites evolutions, petits bugs 
résolus, et sur un autre jeu de données : 5 ...
Au passage je fais une course CPU contre GPU sur le cluster, car je suspecte un problème 
avec les GPUs trop lents ? 
Il semblerait effectivement que le train aille plus vite sur CPU que GPU ! comment est-ce 
possible ? Et en plus sans compilation optimisée pour AVX, ...
Du coup j'essaye de compiler tensorflow pour AVX sur machine locale
La compilation maison de TF a fonctionné, ça va plus vite en effet, j'essaye maintenant de 
compiler pour les E5-2650 Sandybridge du cluster, mais depuis ma machine locale car
je ne peux pas installer bazel sur le cluster.
Complications inattendues : il faut build pour python3.4, qui n'est pas sur ub17.10, donc
il faut bidouiller ça prend du temps.
Entre temps j'implémente kl divergence en plus de la perplexité dans hank.py
C'est un peu le bordel avec mes venvs sur le cluster, il faut que je range tout ça
Tentatives de parallélisations de la génération des mots pour aller plus vite, malheureusement
il y a la difficulté du set, sinon les trucs pas uniques vont tuer le temps que prends le model.
En fait le set ça se passe très bien, on fait aussi les probas en un GROS batch et ça va mieux,
la vitesse est vraiment meilleure, on va pouvoir faire des tests assez gros.

Mercredi 7 Mars : 10h - 18h (ns)
Je passe la matinée a mettre de l'ordre dans les venvs sur le cluster et a essayer de compiler TF
pour les cpus du clusters. Malheureusement plein de problèmes de version de gcc, libcpp, ...
Je fais quelques essais en local pour hank.py mais même après quelques réglages, on a des problèmes
d'échelle : impossible de le faire marcher avec des taille qui laisse une chance de fonctionner.
Je repars à l'attaque de la compilation de TF cette fois en compilant Bazel sur le cluster, pour
pouvoir faire la compilation de TF de là bas (adnvideo1).
Ok compilation de Bazel longue mais ça a l'air bon
J'ai pu lancer compilation de TF CPU, un peu long, on verra.
A part ça, à partir de 3 ou 4 epochs, la perplexité du modèle augmente fortement : surapprentissage?
J'ai essayé de comprendre pourquoi on a autant de zéros, c'est un arrondi, qui se produit encore
plus fréquement sur les longs mots, forcément.

Jeudi 8 Mars : 10h - 18h (ns)
J'arrive en retard car j'ai perdu ma carté de métro. En arrivant je suis déçu de voir que ni la
compilation de TF ni l'expérience avec gros lrows et lcols sur le jeu 5 n'ont fonctionnées.
Je n'ai pas d'autre choix que de relancer (pour la compilation cette fois il semble se passer
vraiment quelquechose, ok).
Cette fois la compilation a réussi, en 10 minutes, je ne sais pas ce qu'il s'est passé hier soir...
Du coup ça y est on a un environnent performant pour mener des expériences a grande échelle !
Ok après expérience, le CPU semble plus rapide pour l'entrainement du modèle, mais pour l'extraction
GPU fait un peu mieux.
Je continue le travail sur l'accélération de hank.py : trops de doublons soumis au rnn, paralléli-
sons l'encodage-prefixage-padding des mots a évaluer.
Mini réunion SA : pointe problèmes de RNN, suggère de vérifier en croisant loss, perp, sur des
pautomacs pour vérifier que ce ne soit pas un problème de RNN inadapté.
Un réorganisation du code me semble nécéssaire à la lumière de mes progrès en Python.
Les travaux sur proba_words sont bons ! sur pautomac5, 200 neuronnes, 3 3 on passe d'un grosse heure
à 7 minutes. ça laisse espérer de pouvoir monter à 4 4, je lance l'expérience, a relancer demain
matin pour le weekend eventuellement.
Après j'améliore test selon les conseils de SA, voir test2.py
Je lance l'expérience sur test2.py, données paut6

Vendredi 9 Mars : 9h45 - 12h
Observations des résultats des expériences.

--> S5 : (7.5)+9+8+8+(2.25) = 34.75

Lundi 12 Mars : 8h30 - 11h30 12h - 17h
Je décortique un peu les résultats des expériences, quelques mauvaises nouvelles :
 - Les apprentissages (hors ceux tués pour libérer des GPUs) même avec 96h ne finiront pas, mais on
a quand même une quinzaine d'epochs sur 20.
 - L'extraction d'automate est morte aussi, je pense que c'est par manque de RAM, à creuser pour
éviter ce problème.
Par de petits calculs, problèmes confirmés : donc on a toujours un problème d'échelle pour la
spectralisation, plus a cause du temps mais à cause de l'espace. A creuser...
Donc : petite amélioration pour pas faire les mêmes calculs pours lig et col si ils ont des dims en
commun, del des trucs qui serviront plus (mieux que le ramasse-miettes ?), Encore un peu plus de par
allélisation, mais ça ne résoud pas les questions d'espace... faut-il passer a un générateur ?
Après études et réflexions, un générateur réduira fortement la contrainte d'espace, j'espère que
ça ne pèsera pas trop sur la contrainte de temps (moins de parallélisme a priori, sauf si TF/Keras
sont des génies et que predict_generator les tire au fur et a mesure du generator, auquel cas ce
sera masqué par le temps de prédiction, ce qui serait génial) et aussi que ça suffira à résoudre le
problème d'espace.

Mardi 13 Mars : 8h-12h 13h-19h45
Coupure electrique aujd 12-13h30 ça m'arrange pas du tout, travail AM maison ?
Hank2 avec générateurs fonctionne maintenant, il reste a tester. ça a été l'occasion de découvrir un
gros bug dans hankel (remplissage des matrices : la epsilon était écrassée par la 1, la dernière
jamais remplie (décalage de 1) oups !), ça élimine tous les zéros qui était remplacés par epsilon,
par contre du coup il y a une moitié de négatifs !
Ce qui est censé multithreader sur le générateur ne marche pas, les args pas picklable ? Bizarre.
Problème de mise en désordre ? Apparament bug connu de Keras mais ils s'en foutent. Certains
suggère de faire à la main en utilisant train_on_batch, mais ça me semble compliqué comme parrallé-
lisation, surtout en rapport avec problème suivant :
Autre gros problème, impossible d'instancier plusieurs modèles à partir d'un même fichier on dirait.
Du coup impossible de paralléliser sur l'alphabet. 
A midi je rentre a la maison pour continuer a bosser, a cause de la coupure d'électricité.
Test de hank2 (avec generator) : Beacoup plus lent (x 2*nalpha)
On peut palier un peu ça en faisant des batchs de batchs =)
Malheureusement, à cause du retour des redondances dans les données a évaluer, un 5 5 devrait avoir
un temps de traitement d'environ 2 mois...
Du coup hank3 hybride des deux j'espère, plutôt rapide, codage/decodage d'un mot en un entier pour
ménager la RAM. Ça à l'air de marcher, je lance cette nuit avec 5 5, on verra demain.
A faire : un mode "rapide" avec raccourcis car lrows et lcols sont égaux et "plein" ?
Je finis tard pour pas lacher l'inspiration sur hank3

Mercredi 14 Mars : 8h - 11h45 12h45 - 16h
Hank3 est lui aussi mystérieusement mort. Manque de mémoire ? bizarre car en interactif ça marchait,
Moins de mémoire en sub qu'en interactif ? En fait en interactif ça a pas remarché (see4c1)
Je vais essayer de faire un hank4 "express" qui tient compte du fait qu'on utilise toujours tous les
mots jusqu'à n.
hank4express.py est né ! Je le teste avec lrows=lcols=5: 300 heures pour l'évaluation des mots
... pénible mais c'est peut-être le prix a payer...
En fait en faisant des gros batch (1000~2000) on tombe a 150 heures, raisonnablement enviseageable.
Voyons si le GRU est plus rapide ? : Non Gru pas plus rapide, environ 140 heures, avec 50 neuronnes
seulement, par contre plus docile ? ou c'est grace a pautomac6 ? > (15 4) donne des résultats
sympas : (66.98, 69.52, 76.83), KL RNN-EXTR = 0.0232. Point positif, la performance semble croitre
avec lrows-lcols (diminution du nombre de négatifs par ailleurs), qui donne de l'espoir pour (15 5)
En attendant les résultats sur (15 4) pour comparer, je regarde le résultat des nombreux apprentis-
sages lancés vendredi. J'ai du en tuer certains pour libérer des GPUs, w8 a planté a cause d'un bug
avec le sous-padding (pas traité à ce jour, car hank3 et hank4 est devenu la norme, se méfier), la 
reférence n'a pas eu le temps d'aller au bout des 20 batchs (96h quand même !). Résultats :
Loss et perplexity ne sont pas corrélés (mais c'était pas attendu qu'ils le soient), psample est bon
en loss mais pas en perplexity, se méfier !
Réunion SA et RE :
C'est normal qu'il y ait des poids négatifs prédits par l'automate extrait, ça devrait diminuer en
augmentant le rang et la taille des préfixes et suffixes, il sera difficile de descendre en dessous
de 5~10% de négatifs.
C'est (peut-être) normal que ça marche mieux sur pauto6 (DPFA) que pauto5 (HMM) en raison de leur na
ture. (Je vais peut-être me cantonner a des exemples dociles pour avoir quelques résultats sympas)
TODO :
Essayer en variant fortement le rang, essayer en sélectionnant aléatoirement un certain nombre de 
préfixes et suffixes, ça devrait être possible en adapdant légèrement hank3. RE doit me donner une 
liste des problèmes pautomac les plus représentatifs d'après B. Balle. RE absent 1.5 semaines après

Jeudi 15 Mars : 9h30 - 18h45 (ns yerba)
Je modifie hank4 pour prendre une liste de rangs a considérer plutôt qu'un unique rang, ça permet de
grouper les fastidieuses opérations de préparation, puis de faire le spectral avec plusieurs valeurs
de rangs pour l'expérience.
Résultat d'une petite expérience : il semblerait que plus on augmente le rang, plus proche est la
perp, mais plus on a de negs (bizarre, contradictoire ?). (Même si on a largement dépassé le vrai
rang).
Je modifie hank3 pour faire un drop aléatoire sur ligs et cols, ça impacte assez fort sur les perfs
pour une longueur fixée, a voir si ça se compense en prenant une longueur plus grande mais en
abandonnant pour rapprocher du temps d'éxécution de hank4. C'est perf/temps qui compte. La mise en
batch est assez douloureuse, mais ça pourrait se paralléliser comme dans hank2.
Les résultats sont hyper mauvais dis-donc ! est-ce que j'applique une réduction trop brutale ? en un
petit quart d'heure ça fait beaucoup moins bien que hank4 en 5 minutes !
hank4 (25 3) en 5 minutes > 75.788, cible 68.621 (176 negs)
hank3 (25 4 4) compréssé au même nombre que 3 (mais ça fait plus de préfixes unique donc long) en
15 minutes environ > 1336.230, cible 68.621 (408 negs)
J'ai essayé avec des valeurs de rangs variables, ça reste désespérément foireux ! et avec plus de
longueur c'est aussi assez catastrophique.
Y a-t-il un bug ? dans certains cas je récupère une KL négative, c'est impossible normalement !
peut-être qu'a cause d'epsilon, on n'a pas une vraie dist de proba, donc les thms sur KL ne marchent
pas.
Expérience : paut6 perp réelle : 66.98, perp rnn : 68.62, taille  3 3,
			 vrai rang : 19, rangs : 8 10 19 20 30 50 100 200
Avec fausse réduction (coeff 1 1) : résultat 84 sur rang 8
								    résultats ~75 +/- 1 sur rangs 10 ... 100
								    résultat 1e+35 sur rang 200
CONFIRMATION par hank4 (sans réduction, exacts mêmes résultats que hank3 avec réduction 1 1)
Avec petite réduction (coeff 0.95 0.95) : a peu près idem (un peu mieux ?)jusqu'à rang 50 inclus, 
										  n'importe quoi total pour 100 et 200
Avec un petit coeff de réduction (95%), on a des résultats semblables a sans réduction jusqu'à rang
50 (perp 74.49, visée 68.62), plus gros rang la perplexité explose (à 100 : 761m) ! Pourtant la KL
continue de descendre ! on voit que nb de probas négatives, perplexité et div KL ne sont pas cor-	
rélés. Alors qui croire ?
Préparation de trains de models sur des problèmes représentatifs (en tout cas moi je les aime) avec
les trois types de machines génératrices représentés et des petits alphabets. Exploration du HW du
cluster pour savoir sur quels noeuds je peux lancer ou non (support AVX2 notamment)
J'en profite pour débug le calcul de perplexité avec des sous-padding (voir ci-dessus) comme ça on
pourra l'utiliser pour entrainer des modèles en surveillant leur perplexité.
Problème : Entre hank 1, 2, 3, 4 trop de duplicatas partiellements maintenus, ça m'énerve. A la lu-
mière de mes progrès en python, la façon dont est faite hank* ne me plait pas, je réorganise tout
joliement avec des classes, des héritages, des surcharges... comme ça on aura le moins possible de
code dupliqué partout qu'il faut corriger et recorriger. On inaugure les Spex (SpectralExtractors).

Vendredi 16 Mars : 9h - 12h
Encore un peu de structuration de hank en spex, lancement des apprentissages.
Réunion du vendredi.

--> S6 : 8+(10.75)+7+(9,25)+3 = 38

Lundi 19 Mars :
Mardi 20 Mars :
Mercredi 21 Mars :
Jeudi 22 Mars :
Vendredi 23 Mars :

Lundi 26 Mars :
Mardi 27 Mars :
Mercredi 28 Mars :
Jeudi 29 Mars :
Vendredi 30 Mars :

Lundi 2 Avril : LUNDI DE PAQUES
Mardi 3 Avril :
Mercredi 4 Avril :
Jeudi 5 Avril :
Vendredi 6 Avril :

Lundi 9 Avril :
Mardi 10 Avril :
Mercredi 11 Avril :
Jeudi 12 Avril :
Vendredi 13 Avril :

Lundi 16 Avril :
Mardi 17 Avril :
Mercredi 18 Avril :
Jeudi 19 Avril :
Vendredi 20 Avril :

Lundi 23 Avril :
Mardi 24 Avril :
Mercredi 25 Avril :
Jeudi 26 Avril :
Vendredi 27 Avril :

Lundi 30 Avril :
Mardi 1 Mai : FETE DU TRAVAIL
Mercredi 2 Mai :
Jeudi 3 Mai :
Vendredi 4 Mai :

Lundi 7 Mai :
Mardi 8 Mai : ARMISTICE 45
Mercredi 9 Mai :
Jeudi 10 Mai : ASCENSION
Vendredi 11 Mai :

Lundi 14 Mai :
Mardi 15 Mai :
Mercredi 16 Mai :
Jeudi 17 Mai : GT?
Vendredi 18 Mai : GT

Lundi 21 Mai : PENTECOTE
Mardi 22 Mai : GT
Mercredi 23 Mai : GT?
Jeudi 24 Mai :
Vendredi 25 Mai :

Lundi 28 Mai :
Mardi 29 Mai :
Mercredi 30 Mai :
Jeudi 31 Mai :
Vendredi 1 Juin :

Lundi 4 Juin :
Mardi 5 Juin :
Mercredi 6 Juin :
Jeudi 7 Juin :
Vendredi 8 Juin :

Lundi 11 Juin :
Mardi 12 Juin :
Mercredi 13 Juin :
Jeudi 14 Juin :
Vendredi 15 Juin :

Lundi 18 Juin :
Mardi 19 Juin :
Mercredi 20 Juin :
Jeudi 21 Juin :
Vendredi 22 Juin :

Lundi 25 Juin :
Mardi 26 Juin :
Mercredi 27 Juin :
Jeudi 28 Juin :
Vendredi 29 Juin :

